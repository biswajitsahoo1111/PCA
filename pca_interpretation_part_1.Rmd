---
title: "pca_interpretation_part_I"
author: "Biswajit Sahoo"
date: "December 10, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
This post is related to Principal Component Analysis (PCA), one of the most popular dimensionality reduction techiques used in machine learing. The appliction of PCA and its variants are ubiqutous. In almost all softwares, such as MATLAB, R, etc., built-in commands are available to perform PCA. In this post we will show how can we obtain results of PCA from raw data without using builtin commands. Then we will reproduce the results of a published paper on PCA that is very popular in academic circles. The post is divided into three parts to meake it managable to read. Readers who are totally familiar with PCA should read none and leave the page immediately. Other readers who are familiar with PCA but want to see different implementations, should jump to the part they wish to read. Absolute beginners should start with Part-I and work their way through gradually. Beginners are also encouraged to explore the references for further information. Here is the outline of different parts:  

* Part-I: Basic Theory of PCA
* Part-II: PCA Implementation with and without using built-in functions
* Part-III: Reproducing results of a published paper on PCA

For Part-II, and Part-III, both MATLAB and R codes are available to reproduce all the results. In this post, let's discuss the theory behind PCA.

Principal Component Analysis (PCA) is one of the most popular dimensionality reduction techniques. Though its origin dates back to early 19th century, it has never gone out of fashion it seems. Its popularity grows steady as can be gaugesd by the number of papers and articles published related to PCA or its variants. 

All popular programming platforms contain built in functions that perform PCA given a data matrix. In this blog we will use the open source statistical programming enviroment R to demonstrate the result.

We will also show how we can obtain the results using simple matrix operations without using the builtin function. 

In this blog we will reproduce using R all the results of an immensely popular paper on PCA by Abdi et. al. The paper got published in 2010 and within 8 years it has got close to 3800 citations. The data will be taken from the
paper itself.

$$\textbf{S} = \frac{1}{n-1}\textbf{X}^T\textbf{X}$$
is the covariance matrix.


```{r}
library(ggplot2)
```

```{r}
x = seq(0,2*pi,length.out = 300)
a = cos(x);b = sin(x)
circ_data = data.frame(a,b)
#ggplot(circ_data,aes(circ_data[,1],circ_data[,2]))+geom_path()

ggplot()+geom_path(data = circ_data,aes(circ_data[,1],circ_data[,2]))
  # geom_point(data = as.data.frame(pca$rotation),aes(x = pca$rotation[,1],y = pca$rotation[,2]),col = "red")+geom_text_repel()
```
```{r, null_prefix = TRUE}
food = read.csv("pca_abdi_food.csv",header= T)
head(food[,3:9])
```
We will breifly state the theory from the paper with slight modification of notation. We will use conventional linear algebra notations against those used in the paper for matrix factorizations.
# This is the change I made.
